{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "env = gym.make('MsPacman-v0')\n",
    "\n",
    "frames = []\n",
    "\n",
    "n_max_steps = 1000\n",
    "n_change_steps = 10\n",
    "\n",
    "obs = env.reset()\n",
    "for step in range(n_max_steps):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    if step % n_change_steps == 0:\n",
    "        action = env.action_space.sample() # play randomly\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)\n",
    "\n",
    "video = plot_animation(frames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Workspace/Checkouts/python-ml/env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n: [(3.0, 5.0), (5.0, 4.0)]\n:: None\n::: [(3.0, 4.7), (5.0, 3.5)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "#y = X[0][0] * X[0][1]\n",
    "#hidden = tf.layers.dense(X, 2, kernel_initializer=initializer)\n",
    "#logits = tf.layers.dense(hidden, 1)\n",
    "\n",
    "X = tf.Variable(5.)\n",
    "Y = tf.Variable(4.)\n",
    "res = X * 3 + Y * 5\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(0.5)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "#cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=[[5.]], logits=hidden)\n",
    "grads_and_vars = optimizer.compute_gradients(res)\n",
    "# gradients = [grad for grad, variable in grads_and_vars]\n",
    "\n",
    "feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    # grad = tf.Variable(0.1)\n",
    "    feed.append((grad, variable))\n",
    "    \n",
    "training_op = optimizer.apply_gradients(feed)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(res))\n",
    "    grads_and_vars_result = sess.run(grads_and_vars, feed_dict={})\n",
    "    print(\":\",grads_and_vars_result)\n",
    "    applied = sess.run(training_op)\n",
    "    print(\"::\",applied)\n",
    "    grads_and_vars_result2 = sess.run(grads_and_vars, feed_dict={})\n",
    "    print(\":::\",grads_and_vars_result2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c3683da0f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdiscount_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdiscounted_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcumulative_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "        print(step)\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    # return flat_rewards\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "print(discount_and_normalize_rewards([[10, 0, -50], [10, 0, -50]], discount_rate=0.8))\n",
    "#discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
